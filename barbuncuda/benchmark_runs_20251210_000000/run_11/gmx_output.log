                      :-) GROMACS - gmx mdrun, 2025.3 (-:

Executable:   /arf/home/ecevik/gromacs/bin/gmx
Data prefix:  /arf/home/ecevik/gromacs
Working dir:  /arf/scratch/ecevik/workspace/benchmark_runs_barbuncuda/barbuncuda/benchmark_runs_barbuncuda/run_11
Command line:
  gmx mdrun -s md_0_250ns_nohmr.tpr -ntmpi 2 -ntomp 12 -nb gpu -pme gpu -bonded gpu -npme 1 -gpu_id 01 -nsteps 50000 -noconfout -dlb yes -notunepme

Reading file md_0_250ns_nohmr.tpr, VERSION 2024.2 (single precision)
Note: file tpx version 133, software tpx version 137
Overriding nsteps with value passed on the command line: 50000 steps, 100 ps
Changing nstlist from 10 to 100, rlist from 1 to 1.17


Update groups can not be used for this system because there are three or more consecutively coupled constraints

On host barbun137 2 GPUs selected for this run.
Mapping of GPU IDs to the 2 GPU tasks in the 2 ranks on this node:
  PP:0,PME:1
PP tasks will do (non-perturbed) short-ranged and most bonded interactions on the GPU
PP task will update and constrain coordinates on the CPU
PME tasks will do all aspects on the GPU
GPU direct communication will be used between MPI ranks.
Using 2 MPI threads
Using 12 OpenMP threads per tMPI thread


NOTE: The number of threads is not equal to the number of (logical) cpus
      and the -pin option is set to auto: will not pin threads to cpus.
      This can lead to significant performance degradation.
      Consider using -pin on (and -pinoffset in case you run multiple jobs).
starting mdrun 'GROup of MAchos and Cynical Suckers in water'
50000 steps,    100.0 ps.

               Core t (s)   Wall t (s)        (%)
       Time:     2016.747       84.036     2399.9
                 (ns/day)    (hour/ns)
Performance:      102.816        0.233

GROMACS reminds you: "Theoretical chemistry has of course always been important and useful ... at least to theoretical chemists" (Sven Lidin)

