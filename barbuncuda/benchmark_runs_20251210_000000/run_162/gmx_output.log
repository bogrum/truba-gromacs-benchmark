                      :-) GROMACS - gmx mdrun, 2025.3 (-:

Executable:   /arf/home/ecevik/gromacs/bin/gmx
Data prefix:  /arf/home/ecevik/gromacs
Working dir:  /arf/scratch/ecevik/workspace/benchmark_runs_barbuncuda/barbuncuda/benchmark_runs_barbuncuda/run_162
Command line:
  gmx mdrun -s md_0_250ns_nohmr.tpr -ntmpi 26 -ntomp 1 -nb gpu -pme gpu -bonded gpu -npme 1 -gpu_id 01 -nsteps 50000 -noconfout -dlb yes -pin on

Reading file md_0_250ns_nohmr.tpr, VERSION 2024.2 (single precision)
Note: file tpx version 133, software tpx version 137
Overriding nsteps with value passed on the command line: 50000 steps, 100 ps
Changing nstlist from 10 to 100, rlist from 1 to 1.17


Update groups can not be used for this system because there are three or more consecutively coupled constraints

On host barbun137 2 GPUs selected for this run.
Mapping of GPU IDs to the 26 GPU tasks in the 26 ranks on this node:
  PP:0,PP:0,PP:0,PP:0,PP:0,PP:0,PP:0,PP:0,PP:0,PP:0,PP:0,PP:0,PP:0,PP:1,PP:1,PP:1,PP:1,PP:1,PP:1,PP:1,PP:1,PP:1,PP:1,PP:1,PP:1,PME:1
PP tasks will do (non-perturbed) short-ranged and most bonded interactions on the GPU
PP task will update and constrain coordinates on the CPU
PME tasks will do all aspects on the GPU
GPU direct communication will be used between MPI ranks.
Using 26 MPI threads
Using 1 OpenMP thread per tMPI thread

starting mdrun 'GROup of MAchos and Cynical Suckers in water'
50000 steps,    100.0 ps.


Dynamic load balancing report:
 DLB was permanently on during the run per user request.
 Average load imbalance: 14.3%.
 The balanceable part of the MD step is 51%, load imbalance is computed from this.
 Part of the total run time spent waiting due to load imbalance: 7.2%.
 Steps where the load balancing was limited by -rdd, -rcon and/or -dds: X 0 % Y 0 %
 Average PME mesh/force load: 3.381
 Part of the total run time spent waiting due to PP/PME imbalance: 42.5 %

NOTE: 7.2 % of the available CPU time was lost due to load imbalance
      in the domain decomposition.
      You can consider manually changing the decomposition (option -dd);
      e.g. by using fewer domains along the box dimension in which there is
      considerable inhomogeneity in the simulated system.
NOTE: 42.5 % performance was lost because the PME ranks
      had more work to do than the PP ranks.
      You might want to increase the number of PME ranks
      or increase the cut-off and the grid spacing.


               Core t (s)   Wall t (s)        (%)
       Time:     2213.220       85.131     2599.8
                 (ns/day)    (hour/ns)
Performance:      101.492        0.236

GROMACS reminds you: "Your assumptions are your windows on the world. Scrub them off every once in a while, or the light won't come in." (Isaac Asimov)

